{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMd/Qc01VKvGjQrmrDVTNtX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WOscarG/Arquitectura-red-LSTM/blob/main/ARQUITECTURA_RED_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VL2NGuHu0LYH"
      },
      "outputs": [],
      "source": [
        "Diseño de la arquitecra de la red LSTM - para la predicción de la demanda eléctrica a corto plazo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM_E_1(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length, num_dir, device):\n",
        "        super(LSTM_E_1, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes #number of outputs\n",
        "        self.num_layers = num_layers #number of layers\n",
        "        self.input_size = input_size #input size\n",
        "        self.hidden_size = hidden_size #hidden state\n",
        "        self.seq_length = seq_length #sequence length\n",
        "        self.num_directions = num_dir\n",
        "\n",
        "        if num_dir == 2:\n",
        "            bi = True\n",
        "        else:\n",
        "            bi = False\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True, bidirectional = bi ) #lstm\n",
        "        self.fc_1 =  nn.Linear(hidden_size*num_dir, 100) #fully connected 1\n",
        "        self.fc = nn.Linear(100, num_classes) #fully connected last layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        h_0 = Variable(torch.zeros(\n",
        "              self.num_layers*self.num_directions, x.size(0), self.hidden_size)).to(self.device) #hidden state\n",
        "        c_0 = Variable(torch.zeros(\n",
        "            self.num_layers*self.num_directions, x.size(0), self.hidden_size)).to(self.device) #internal state\n",
        "\n",
        "\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "\n",
        "        out = output.view(-1, self.hidden_size*self.num_directions) #reshaping the data for Dense layer next\n",
        "\n",
        "        out = self.relu(out)\n",
        "        out = self.fc_1(out) #first Dense\n",
        "        out = self.relu(out) #relu\n",
        "        out = self.fc(out) #Final Output\n",
        "\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Xp5qy9IB0dQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "if os.path.isfile('./data/models/model_1_checkpoint.tar'):\n",
        "\n",
        "    model = LSTM_E_1(num_classes = 1, input_size = X_train_tensors_final.shape[2], hidden_size=133, num_layers=1, seq_length=X_train_tensors_final.shape[1],  num_dir = 1, device=device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "    checkpoint = torch.load('./data/models/model_1_checkpoint.tar', map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    print(model)\n",
        "\n",
        "else:\n",
        "    model = LSTM_E_1(num_classes = 1, input_size = X_train_tensors_final.shape[2], hidden_size=133, num_layers=1, seq_length=X_train_tensors_final.shape[1],  num_dir = 1, device=device)\n",
        "    model.to(device)\n",
        "    print(model)"
      ],
      "metadata": {
        "id": "YdNX3qch0hPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ia27eptu0PKM"
      }
    }
  ]
}